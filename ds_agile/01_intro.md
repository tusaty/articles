## 1. Intro

I've started this article, since this is great way to summarise the experience and look on it from different angles.

Major fails of the projects I had a chance to participate are associated with leak of motivation, leak of clear targets and leak of responsibility. I think that's a typical picture.

There is a typical pain point for data science projects - responsibility shared between product owner and data science team. Goals are setup by product owner / offering manager, but they need to be re-validated and corrected by science teams. This forms a loop of two major parts: giving assumptions and re-validating assumptions.

Also responsibility leak can be amplified by typical matrix system, where team is responsible to deliver several projects in time. This leads to providing non-thoroughful assumptions and non-valid priorities assumptions.

In this part I would like go through most typical failures of data science projects:


### 1.1 Einstellung.
I think that could happen with many teams. Once first solution appears, team starts work on improvements. At some time point team end ups trying different configurations / tweaks which doesn't bring much improvements in lieu to the time spent. Team lead should recognise such situations and help his team to make a switch to brainstorming new approaches and ideas.


### 1.2 Non-value driven
At some point it might appear that current data doesn't produce much value to the client.
E.g. looking into performance anomalies doesn't bring a high value,
if client doesn't care about his data. Business needs insights which bring benefits.
How much revenue / savings does your idea bring?

###  1.3 No interaction with end users
This is very similar to previous point. Any value we assume, should be justified by the customer. Justified by his feedback. If team fails to find interested/sponsored client and re-validate value assumptions, then most probably there is not so much value in the use-case.
Probably project can be de-prioritised/postponed.

### 1.4 Following non-valid plan
That's similar to Einstellung point. In data science projects the only plan is no plan. They are driven by very cohessed interaction in between product owners, data scientist and clients.
Plan should be easy to adopt, tasks should have clear acceptance criteria. Any subjective judgement should be eliminated from acceptance criteria formulation.


### 1.5 Hard limits not defined
Hard limits are good to stop current activity and re-assess values and goals.
If hard limits are not defined, this will likely result in constant research-improvement loop without any product produced. Thus boxing each R&D process into sprints should make the things. Each sprint should result in minimal viable product (MVP). Such approach helps to prevent indeterminate research issue, which I meet very often across data science teams.

### 1.6 Not iterative
Each data science process should be iterative. Mostly this is determined by leak of information on each step. Each iteration reduces entropy about target goal by assumptions re-validation. By the way I don't know yet what to do with those iteration which produce more entropy, I think this is a signal to stop and re-validate goals setting.

### 1.7 Motivation loss
Motivation is everywhere in data science projects. First of all: data scientists, which are usually highly motivated by:
  a. learning new things
  b. feeling themselves on the edge of modern technology
  c. bringing real value
It's very likely that non-motivated data-scientist won't produce any insights or meaningful analysis.

### 1.8  Excuse for ad-hoc behaviour
Each team should have a discipline and some minimal ethics about work and delivering product to the customer. Being agile doesn't mean that team can short cut on tasks definitions or QA processes and push new code directly to the customer.
